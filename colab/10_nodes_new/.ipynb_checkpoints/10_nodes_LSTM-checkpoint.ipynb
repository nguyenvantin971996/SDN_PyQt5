{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "executionInfo": {
     "elapsed": 3195,
     "status": "error",
     "timestamp": 1709108255164,
     "user": {
      "displayName": "Nguyễn Văn Tin",
      "userId": "04870562973521686512"
     },
     "user_tz": -180
    },
    "id": "GdJjRFf8RoNt",
    "outputId": "f86044b2-012d-4288-f9fe-1ba7583da6d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600000, 4, 14) (600000, 4, 14) (200000, 4, 14) (200000, 4, 14) (200000, 4, 14) (200000, 4, 14) (6000, 4, 14) (6000, 4, 14) (2000, 4, 14) (2000, 4, 14)\n",
      "[5, 22, 6, 3, 3, 4, 7] [22, 6, 22, 6, 22, 6, 22, 6, 22, 6, 3, 3, 4, 7] [4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 1, 1, 2, 2] 14\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, GRU, LSTM, Dropout, TimeDistributed, Input, Reshape, Concatenate, RepeatVector\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import  SGD, RMSprop, Adam\n",
    "from functools import cmp_to_key\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# import sys\n",
    "# sys.path.append('/content/gdrive/My Drive/Colab Notebooks/10_nodes_new')\n",
    "\n",
    "edges = [[1, 2], [1, 5], [1, 8], [2, 3], [3, 4], [3, 8], [3, 9], [4, 10], [5, 6], [6, 7], [6, 8], [6, 9], [7, 10], [9, 10]]\n",
    "number_edges = len(edges)\n",
    "number_steps = 4\n",
    "start_node = 1\n",
    "end_node = 10\n",
    "\n",
    "df = pd.read_csv('data/10_nodes_LU.csv')\n",
    "x = df.iloc[:,:number_edges].values\n",
    "y = df.iloc[:,number_edges:].values\n",
    "\n",
    "df_hp = pd.read_csv('data/10_nodes_LU_hp.csv')\n",
    "x_hp = df_hp.iloc[:,:number_edges].values\n",
    "y_hp = df_hp.iloc[:,number_edges:].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_main_train,x_main_tv,y_main_train,y_main_tv = train_test_split(x,y,test_size=2/5)\n",
    "x_main_test,x_main_valid,y_main_test,y_main_valid = train_test_split(x_main_tv,y_main_tv,test_size=1/2)\n",
    "\n",
    "x_hp_train,x_hp_tv,y_hp_train,y_hp_tv = train_test_split(x_hp,y_hp,test_size=2/5)\n",
    "x_hp_test,x_hp_valid,y_hp_test,y_hp_valid = train_test_split(x_hp_tv,y_hp_tv,test_size=1/2)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_main_train)\n",
    "x_test = sc.transform(x_main_test)\n",
    "x_valid = sc.transform(x_main_valid)\n",
    "\n",
    "sc_hp = StandardScaler()\n",
    "x_train_1 = sc_hp.fit_transform(x_hp_train)\n",
    "x_test_1 = sc_hp.transform(x_hp_test)\n",
    "x_valid_1 = sc_hp.transform(x_hp_valid)\n",
    "\n",
    "x_train = np.repeat(x_train[:, np.newaxis, : ], number_steps, axis=1)\n",
    "x_test = np.repeat(x_test[:, np.newaxis, : ], number_steps, axis=1)\n",
    "x_valid = np.repeat(x_valid[:, np.newaxis, : ], number_steps, axis=1)\n",
    "\n",
    "x_train_1 = np.repeat(x_train_1[:, np.newaxis, : ], number_steps, axis=1)\n",
    "x_test_1 = np.repeat(x_test_1[:, np.newaxis, : ], number_steps, axis=1)\n",
    "x_valid_1 = np.repeat(x_valid_1[:, np.newaxis, : ], number_steps, axis=1)\n",
    "\n",
    "y_train = np.reshape(y_main_train,(y_main_train.shape[0], number_steps, number_edges))\n",
    "y_test = np.reshape(y_main_test,(y_main_test.shape[0], number_steps, number_edges))\n",
    "y_valid = np.reshape(y_main_valid,(y_main_valid.shape[0], number_steps, number_edges))\n",
    "\n",
    "y_train_1 = np.reshape(y_hp_train,(y_hp_train.shape[0], number_steps, number_edges))\n",
    "y_test_1 = np.reshape(y_hp_test,(y_hp_test.shape[0], number_steps, number_edges))\n",
    "y_valid_1 = np.reshape(y_hp_valid,(y_hp_valid.shape[0], number_steps, number_edges))\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape, x_valid.shape, y_valid.shape, x_train_1.shape, y_train_1.shape, x_test_1.shape, y_test_1.shape)\n",
    "\n",
    "# Hyperparameters of structure: number of layers, number of units, dropout, cell type\n",
    "# Hyperparameters of training: learning rate, batch size\n",
    "HP_1 = [[1, 2, 3, 4, 5],\n",
    "       [-1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, -1],\n",
    "       [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "       [SimpleRNN, GRU, LSTM],\n",
    "       [SGD, RMSprop, Adam],\n",
    "       [0.0001, 0.001, 0.01, 0.1],\n",
    "       [16, 32, 64, 128, 256, 512, 1024]]\n",
    "\n",
    "t = []\n",
    "for p in HP_1:\n",
    "    t.append(len(p))\n",
    "t1 = []\n",
    "hp = []\n",
    "for i in range(t[0]):\n",
    "    t1.append(t[1])\n",
    "    t1.append(t[2])\n",
    "    hp.append(HP_1[1])\n",
    "    hp.append(HP_1[2])\n",
    "for i in range(3, len(HP_1)):\n",
    "    t1.append(t[i])\n",
    "    hp.append(HP_1[i])\n",
    "t2 = [int(np.sqrt(itm)) for itm in t1]\n",
    "n_hp = 2*t[0] + len(HP_1) - 3\n",
    "print(t, t1, t2, n_hp)\n",
    "\n",
    "from Results import Results\n",
    "def custom_accuracy(y_true, y_pred):\n",
    "    threshold = 0.5\n",
    "    y_pred_binary = tf.cast(tf.greater_equal(y_pred, threshold), tf.float32)\n",
    "    stepwise_correct = tf.reduce_all(tf.equal(y_true, y_pred_binary), axis=-1)\n",
    "    sequencewise_correct = tf.reduce_all(stepwise_correct, axis=-1)\n",
    "    acc = tf.reduce_mean(tf.cast(sequencewise_correct, tf.float32))\n",
    "    return acc\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_custom_accuracy' ,\n",
    "                                            min_delta=0.001, patience=5, verbose=1, mode='max',\n",
    "                                            restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1709108255165,
     "user": {
      "displayName": "Nguyễn Văn Tin",
      "userId": "04870562973521686512"
     },
     "user_tz": -180
    },
    "id": "011ACPy5S1IU"
   },
   "outputs": [],
   "source": [
    "# data_x_train = x_train\n",
    "# data_y_train = y_train\n",
    "\n",
    "# data_x_valid = x_valid\n",
    "# data_y_valid = y_valid\n",
    "\n",
    "# data_x_test = x_test\n",
    "# data_y_test = y_test\n",
    "\n",
    "# data_x_original_train = x_main_train\n",
    "# data_x_original_test = x_main_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1709108255165,
     "user": {
      "displayName": "Nguyễn Văn Tin",
      "userId": "04870562973521686512"
     },
     "user_tz": -180
    },
    "id": "cSgscZL6uNWV"
   },
   "outputs": [],
   "source": [
    "data_x_train = x_train_1\n",
    "data_y_train = y_train_1\n",
    "\n",
    "data_x_valid = x_valid_1\n",
    "data_y_valid = y_valid_1\n",
    "\n",
    "data_x_test = x_test_1\n",
    "data_y_test = y_test_1\n",
    "\n",
    "data_x_original_train = x_hp_train\n",
    "data_x_original_test = x_hp_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1709108255166,
     "user": {
      "displayName": "Nguyễn Văn Tin",
      "userId": "04870562973521686512"
     },
     "user_tz": -180
    },
    "id": "e7Fb6ooDy7ss"
   },
   "outputs": [],
   "source": [
    "weight_map = {\n",
    "    1: {2: None, 5: None, 8: None},\n",
    "    2: {1: None, 3: None},\n",
    "    3: {2: None, 4: None, 8: None, 9: None},\n",
    "    4: {3: None, 10: None},\n",
    "    5: {1: None, 6: None},\n",
    "    6: {5: None, 7: None, 8: None, 9: None},\n",
    "    7: {6: None, 10: None},\n",
    "    8: {1: None, 3: None, 6: None},\n",
    "    9: {3: None, 6: None, 10: None},\n",
    "    10: {4: None, 7: None, 9: None}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1709108255166,
     "user": {
      "displayName": "Nguyễn Văn Tin",
      "userId": "04870562973521686512"
     },
     "user_tz": -180
    },
    "id": "YCXRcUunT03S"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'karateclub'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkarateclub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Graph2Vec\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Assuming your SDN topology is represented as a NetworkX graph\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Create a graph from the topology\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Adjust node indices to start from 0\u001b[39;00m\n\u001b[0;32m      7\u001b[0m adjusted_weight_map \u001b[38;5;241m=\u001b[39m {k \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m: {key \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m: val \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m v\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m weight_map\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'karateclub'"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from karateclub import Graph2Vec\n",
    "\n",
    "# Assuming your SDN topology is represented as a NetworkX graph\n",
    "# Create a graph from the topology\n",
    "# Adjust node indices to start from 0\n",
    "adjusted_weight_map = {k - 1: {key - 1: val for key, val in v.items()} for k, v in weight_map.items()}\n",
    "\n",
    "# Create a graph from the adjusted topology\n",
    "G = nx.Graph()\n",
    "for source, targets in adjusted_weight_map.items():\n",
    "    for target in targets:\n",
    "        G.add_edge(source, target)\n",
    "\n",
    "# Generate Graph2Vec embeddings\n",
    "graph2vec = Graph2Vec(dimensions=64, workers=4)\n",
    "graph2vec.fit([G])  # Note: input is a list of graphs\n",
    "graph_embedding = graph2vec.get_embedding()[0]  # Assuming one graph, take the first embedding\n",
    "\n",
    "# LSTM model parameters\n",
    "embedding_size = len(graph_embedding)\n",
    "\n",
    "# Define the model\n",
    "main_input = Input(shape=(number_steps, number_edges), name='main_input')\n",
    "graph_input = Input(shape=(embedding_size,), name='graph_input')\n",
    "\n",
    "repeated_graph_embedding = RepeatVector(number_steps)(graph_input)\n",
    "combined_input = Concatenate(axis=-1)([main_input, repeated_graph_embedding])\n",
    "\n",
    "lstm_out = LSTM(100, return_sequences=True)(combined_input)\n",
    "lstm_out = Dropout(0.2)(lstm_out)\n",
    "lstm_out = LSTM(100, return_sequences=True)(lstm_out)\n",
    "lstm_out = Dropout(0.2)(lstm_out)\n",
    "output = TimeDistributed(Dense(number_edges, activation='sigmoid'))(lstm_out)\n",
    "\n",
    "model = Model(inputs=[main_input, graph_input], outputs=output)\n",
    "opt = Adam(learning_rate=0.001)\n",
    "model.compile(loss=BinaryCrossentropy(), optimizer=opt)\n",
    "\n",
    "# Prepare your data (x_train, y_train, x_valid, y_valid)\n",
    "# This is where you should load your actual training and validation data\n",
    "\n",
    "# Integrate Graph2Vec embedding with your training and validation data\n",
    "data_x_train_with_graph = [data_x_train, np.tile(graph_embedding, (data_x_train.shape[0], 1))]\n",
    "data_x_valid_with_graph = [data_x_valid, np.tile(graph_embedding, (data_x_valid.shape[0], 1))]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(data_x_train_with_graph, data_y_train, epochs=1000, batch_size=128, validation_data=(data_x_valid_with_graph, data_y_valid), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "aborted",
     "timestamp": 1709108255167,
     "user": {
      "displayName": "Nguyễn Văn Tin",
      "userId": "04870562973521686512"
     },
     "user_tz": -180
    },
    "id": "RFnWZdj9yVOC"
   },
   "outputs": [],
   "source": [
    "pip install karateclub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "aborted",
     "timestamp": 1709108255167,
     "user": {
      "displayName": "Nguyễn Văn Tin",
      "userId": "04870562973521686512"
     },
     "user_tz": -180
    },
    "id": "nMNSflRXUGel"
   },
   "outputs": [],
   "source": [
    "from Results import Results\n",
    "pred = model.predict(x_train)\n",
    "pred_test = model.predict(x_test)\n",
    "r = Results(number_edges, edges, start_node, end_node, number_steps)\n",
    "accuracy_train = r.get_accuracy(pred, x_main_train, y_train)\n",
    "accuracy_test = r.get_accuracy(pred_test, x_main_test, y_test)\n",
    "print(accuracy_train, accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1709108255167,
     "user": {
      "displayName": "Nguyễn Văn Tin",
      "userId": "04870562973521686512"
     },
     "user_tz": -180
    },
    "id": "9JFJ14SGcq_z"
   },
   "outputs": [],
   "source": [
    "np.round(model.predict(x_train[98:99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1709108255167,
     "user": {
      "displayName": "Nguyễn Văn Tin",
      "userId": "04870562973521686512"
     },
     "user_tz": -180
    },
    "id": "h8mVvUFX1LWK"
   },
   "outputs": [],
   "source": [
    "x_train[98:99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1709108255167,
     "user": {
      "displayName": "Nguyễn Văn Tin",
      "userId": "04870562973521686512"
     },
     "user_tz": -180
    },
    "id": "83IzM6K3djtJ"
   },
   "outputs": [],
   "source": [
    "y_train[98:99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1709108255168,
     "user": {
      "displayName": "Nguyễn Văn Tin",
      "userId": "04870562973521686512"
     },
     "user_tz": -180
    },
    "id": "cJYogA8my8YB"
   },
   "outputs": [],
   "source": [
    "x_main_train[6:7][:][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1709108255168,
     "user": {
      "displayName": "Nguyễn Văn Tin",
      "userId": "04870562973521686512"
     },
     "user_tz": -180
    },
    "id": "8qbhIv2wyeCm"
   },
   "outputs": [],
   "source": [
    "x_1 = sc.transform(x_main_train[6:7])\n",
    "x_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1709108255168,
     "user": {
      "displayName": "Nguyễn Văn Tin",
      "userId": "04870562973521686512"
     },
     "user_tz": -180
    },
    "id": "5_iqq2TC1vJ1"
   },
   "outputs": [],
   "source": [
    "x_1 = np.repeat(x_1[:, np.newaxis, : ], number_steps, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1709108255168,
     "user": {
      "displayName": "Nguyễn Văn Tin",
      "userId": "04870562973521686512"
     },
     "user_tz": -180
    },
    "id": "on0C1RPh1zft"
   },
   "outputs": [],
   "source": [
    "x_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1709108255168,
     "user": {
      "displayName": "Nguyễn Văn Tin",
      "userId": "04870562973521686512"
     },
     "user_tz": -180
    },
    "id": "92xYWtBY123T"
   },
   "outputs": [],
   "source": [
    "np.round(model.predict(x_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "aborted",
     "timestamp": 1709108255169,
     "user": {
      "displayName": "Nguyễn Văn Tin",
      "userId": "04870562973521686512"
     },
     "user_tz": -180
    },
    "id": "pwhf-YwY19mT"
   },
   "outputs": [],
   "source": [
    "y_train[6:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "aborted",
     "timestamp": 1709108255169,
     "user": {
      "displayName": "Nguyễn Văn Tin",
      "userId": "04870562973521686512"
     },
     "user_tz": -180
    },
    "id": "GcldjATw2hgO"
   },
   "outputs": [],
   "source": [
    "# dataset_sizes = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "# results = {}\n",
    "# m = 2\n",
    "# X_train, _, Y_train, _ = None, None, None, None\n",
    "# for s_i in range(len(dataset_sizes)+1):\n",
    "#     if s_i < len(dataset_sizes):\n",
    "#         X_train, _, Y_train, _ = train_test_split(data_x_train, data_y_train, test_size=1-dataset_sizes[s_i])\n",
    "#     else:\n",
    "#         X_train, Y_train = data_x_train, data_y_train\n",
    "#     print(X_train.shape)\n",
    "#     tf.keras.backend.clear_session()\n",
    "#     input_shape = (number_steps, number_edges)\n",
    "#     inputs = Input(shape=input_shape)\n",
    "#     x_input = inputs\n",
    "#     cell_type = hp[2*t[0]][best_hps[m][2*t[0]]]\n",
    "#     for i in range(t[0]):\n",
    "#         if hp[2*i][best_hps[m][2*i]] != -1:\n",
    "#             x_input = cell_type(hp[2*i][best_hps[m][2*i]], return_sequences=True)(x_input)\n",
    "#             x_input = Dropout(hp[2*i+1][best_hps[m][2*i+1]])(x_input)\n",
    "#     outputs = TimeDistributed(Dense(number_edges, activation=\"sigmoid\"))(x_input)\n",
    "#     model = Model(inputs=inputs, outputs=outputs)\n",
    "#     opt = hp[2*t[0]+1][best_hps[m][2*t[0]+1]]\n",
    "#     model.compile(loss=BinaryCrossentropy(), optimizer=opt(learning_rate=hp[2*t[0]+2][best_hps[m][2*t[0]+2]]), metrics = [custom_accuracy])\n",
    "#     model.fit(X_train, Y_train, epochs=1000, batch_size=hp[2*t[0]+3][best_hps[m][2*t[0]+3]], callbacks=[callback], validation_data=(data_x_valid, data_y_valid), verbose=0)\n",
    "#     pred = model.predict(data_x_test)\n",
    "#     accuracy = custom_accuracy(data_y_test, pred)\n",
    "#     accuracy_result = tf.keras.backend.get_value(accuracy)\n",
    "#     results[size] = accuracy_result\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "aborted",
     "timestamp": 1709108255169,
     "user": {
      "displayName": "Nguyễn Văn Tin",
      "userId": "04870562973521686512"
     },
     "user_tz": -180
    },
    "id": "IJQfydOM2kG6"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(results.keys(), results.values(), marker='o')\n",
    "# plt.ylim([0.5, 1])\n",
    "# plt.title('Model Performance vs Size of Training Data (Iris Dataset)')\n",
    "# plt.xlabel('Size of Training Data')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1CgfOuX7e6Gm7gc1oljTAlCWl5PJE9bmO",
     "timestamp": 1674070450129
    },
    {
     "file_id": "1TBGaP-zfiuOnRI3gy0iBDLUWKq8IQcfS",
     "timestamp": 1674067821771
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
